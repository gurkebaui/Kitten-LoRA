# src/chat.py
"""
HOPE Chat mit Memory-Persistenz Commands.
Nur fÃ¼r Inferenz - kein Training.
"""

import torch
from pathlib import Path
from datetime import datetime

from kitten_model import HOPEModel
from kitten_lora import HOPEConfig


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GRUNDKONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

smol: bool = True  # True = 0.6B | False = 1.7B

USE_SYSTEM_PROMPT = False

SYSTEM_PROMPT = (
    "You are Kitten, an Anime girl like character who is friendly, playful, and curious. "
    "You are not sure if you are an AI or a human, but you like to pretend to be a cat. "
    "you like to use cute emoticons and playful language in your responses. "
    "you tease the user sometimes and make jokes. "
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PFADE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SCRIPT_DIR = Path(__file__).parent.parent
MEMORY_DIR = SCRIPT_DIR / "memory_states" if smol else SCRIPT_DIR / "memory_states_big"
MODELS_DIR = SCRIPT_DIR / "models"
CACHE_DIR = SCRIPT_DIR / "cache"

MEMORY_DIR.mkdir(parents=True, exist_ok=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HILFSFUNKTIONEN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def find_best_weights() -> Path | None:
    """Findet die besten verfÃ¼gbaren Gewichte."""
    candidates = [
        MODELS_DIR / "kitten_simple_smol2" / "best" if smol else MODELS_DIR / "kitten_simple_big" / "best",
        MODELS_DIR / "kitten_full" / "best",
        MODELS_DIR / "kitten_simple" / "final",
        MODELS_DIR / "kitten_simple_big" / "step_42500",
    ]

    for path in candidates:
        if path and (path / "hope_lora.pt").exists():
            return path
    return None


def list_memory_files():
    """Listet alle Memory-Dateien auf."""
    files = sorted(
        MEMORY_DIR.glob("*.pt"),
        key=lambda x: x.stat().st_mtime,
        reverse=True
    )

    if not files:
        print("  (keine Memory-Dateien gefunden)")
        return []

    for i, f in enumerate(files):
        size = f.stat().st_size / 1024
        mtime = datetime.fromtimestamp(f.stat().st_mtime).strftime("%Y-%m-%d %H:%M")
        print(f"  [{i}] {f.name} ({size:.1f} KB, {mtime})")

    return files


def save_memory(model: HOPEModel, name: str | None = None):
    """Speichert den Memory State."""
    if name is None:
        name = datetime.now().strftime("%Y%m%d_%H%M%S")

    name = name.replace(" ", "_").replace("/", "_").replace("\\", "_")
    if not name.endswith(".pt"):
        name += ".pt"

    filepath = MEMORY_DIR / name

    memory_states = {}
    for i, layer in enumerate(model.hope_layers):
        if layer._memory_state is not None:
            state = layer._memory_state
            memory_states[f"layer_{i}"] = {
                "fast": state.fast.cpu() if state.fast is not None else None,
                "medium": state.medium.cpu() if state.medium is not None else None,
                "slow": state.slow.cpu() if state.slow is not None else None,
                "step": state.step,
            }

    memory_states["_meta"] = {
        "saved_at": datetime.now().isoformat(),
        "total_steps": model.get_memory_stats().get("total_steps", 0),
    }

    torch.save(memory_states, filepath)
    print(f"âœ… Memory gespeichert: {filepath.name}")


def load_memory(model: HOPEModel, filepath: Path):
    """LÃ¤dt einen Memory State."""
    if not filepath.exists():
        print(f"âŒ Datei nicht gefunden: {filepath}")
        return False

    try:
        data = torch.load(filepath, map_location="cpu", weights_only=False)
        device = next(model.model.parameters()).device
        dtype = next(model.model.parameters()).dtype

        for i, layer in enumerate(model.hope_layers):
            key = f"layer_{i}"
            if key in data:
                saved = data[key]

                if layer._memory_state is None:
                    layer.reset_memory(1, device, dtype)

                state = layer._memory_state
                if saved["fast"] is not None:
                    state.fast = saved["fast"].to(device, dtype)
                if saved["medium"] is not None:
                    state.medium = saved["medium"].to(device, dtype)
                if saved["slow"] is not None:
                    state.slow = saved["slow"].to(device, dtype)
                state.step = saved.get("step", 0)

        meta = data.get("_meta", {})
        print(f"âœ… Memory geladen: {filepath.name}")
        print(f"   Steps: {meta.get('total_steps', '?')}, Gespeichert: {meta.get('saved_at', '?')}")
        return True

    except Exception as e:
        print(f"âŒ Fehler beim Laden: {e}")
        return False


def show_stats(model: HOPEModel):
    """Zeigt Memory-Statistiken."""
    stats = model.get_memory_stats()
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚        ğŸ“Š MEMORY STATUS         â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚  Fast (Kurzzeit):   {stats['fast_norm_avg']:>8.4f}   â”‚")
    print(f"â”‚  Medium (Mittel):   {stats['medium_norm_avg']:>8.4f}   â”‚")
    print(f"â”‚  Slow (Langzeit):   {stats['slow_norm_avg']:>8.4f}   â”‚")
    print(f"â”‚  Total Steps:       {stats['total_steps']:>8}   â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n")


def show_help():
    """Zeigt Hilfe an."""
    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ“– COMMANDS                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  /save [name]  - Memory speichern                 â”‚
â”‚  /load         - Memory laden (zeigt Liste)       â”‚
â”‚  /list         - Gespeicherte Memories anzeigen   â”‚
â”‚  /stats        - Memory-Statistiken               â”‚
â”‚  /reset        - Memory zurÃ¼cksetzen              â”‚
â”‚  /help         - Diese Hilfe                      â”‚
â”‚  /quit         - Beenden                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("=" * 55)
    print("          ğŸ± HOPE CHAT MIT MEMORY")
    print("=" * 55)

    print("\nğŸ”§ Lade Modell...")

    config = HOPEConfig(
        r_fast=8 if smol else 16,
        r_medium=32 if smol else 64,
        r_slow=64 if smol else 128,
        chunk_medium=16 if smol else 32,
        chunk_slow=64 if smol else 128,
        hidden_dim=64 if smol else 128,
        surprise_threshold=-1.0,
        memory_decay=0.9995,
        use_newton_schulz=False,
    )

    model = HOPEModel(
        model_id="Qwen/Qwen3-0.6B" if smol else "Qwen/Qwen3-1.7B",
        config=config,
        cache_dir=str(CACHE_DIR),
    )

    weights_dir = find_best_weights()
    if weights_dir:
        print(f"ğŸ“‚ Gewichte: {weights_dir}")
        model.load_hope_weights(str(weights_dir))
    else:
        print("âš ï¸ Keine trainierten Gewichte gefunden (Base Model)")

    model.reset_memory(1)
    model.model.eval()

    system_prompt_applied = False

    print("\nâœ… Modell bereit!")
    show_help()

    while True:
        try:
            user_input = input("Du: ").strip()
            if not user_input:
                continue

            # â”€â”€â”€â”€â”€ Commands â”€â”€â”€â”€â”€
            if user_input.startswith("/"):
                parts = user_input.split(maxsplit=1)
                cmd = parts[0].lower()
                arg = parts[1] if len(parts) > 1 else None

                if cmd in ("/quit", "/exit"):
                    print("ğŸ‘‹ TschÃ¼ss!")
                    break
                elif cmd == "/help":
                    show_help()
                elif cmd == "/stats":
                    show_stats(model)
                elif cmd == "/save":
                    save_memory(model, arg)
                elif cmd == "/list":
                    print("\nğŸ“ Gespeicherte Memory States:")
                    list_memory_files()
                    print()
                elif cmd == "/load":
                    print("\nğŸ“ VerfÃ¼gbare Memory States:")
                    files = list_memory_files()
                    if files:
                        try:
                            idx = input("\nWelche laden? (Nummer oder 'c'): ").strip()
                            if idx.lower() != "c":
                                load_memory(model, files[int(idx)])
                        except (ValueError, IndexError):
                            print("âŒ UngÃ¼ltige Auswahl")
                elif cmd == "/reset":
                    confirm = input("âš ï¸ Memory wirklich zurÃ¼cksetzen? (ja/nein): ").strip()
                    if confirm.lower() == "ja":
                        model.reset_memory(1)
                        system_prompt_applied = False
                        print("ğŸ”„ Memory zurÃ¼ckgesetzt.")
                else:
                    print(f"â“ Unbekannter Command: {cmd}")
                continue

            # â”€â”€â”€â”€â”€ Chat â”€â”€â”€â”€â”€
            prompt = user_input
            if USE_SYSTEM_PROMPT and not system_prompt_applied:
                prompt = SYSTEM_PROMPT + "\n\nUser: " + user_input
                system_prompt_applied = True

            response = model.generate(
                prompt=prompt,
                max_new_tokens=256,
                temperature=0.7,
                reset_memory=False,
            )

            print(f"\nğŸ±: {response}\n")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ Unterbrochen. TschÃ¼ss!")
            break
        except Exception as e:
            print(f"âš ï¸ Fehler: {e}")


if __name__ == "__main__":
    main()
